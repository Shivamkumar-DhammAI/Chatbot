# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bjOefEuBU7palboVTXN2FpdgBuzLW0sY
"""

import streamlit as st
import os
from dotenv import load_dotenv
from langchain.text_splitter import CharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# Load environment variables
load_dotenv()

# Retrieve Google API Key from environment variable
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not GOOGLE_API_KEY:
    st.error("Google API Key is missing! Set it as an environment variable.")
    st.stop()

# Path to the cleaned transcript file
TRANSCRIPT_FILE = "cleaned_transcript.txt"

def load_transcript():
    """Load the cleaned transcript file."""
    if not os.path.exists(TRANSCRIPT_FILE):
        st.error(f"Transcript file '{TRANSCRIPT_FILE}' not found!")
        return ""

    with open(TRANSCRIPT_FILE, "r", encoding="utf-8") as f:
        return f.read().strip()

def get_text_chunks(text):
    """Split text into smaller chunks for better retrieval."""
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    return text_splitter.split_text(text)

def get_vectorstore(text_chunks):
    """Create FAISS vector database from text chunks."""
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004",
        api_key=GOOGLE_API_KEY  # Use API key from environment variable
    )
    return FAISS.from_texts(texts=text_chunks, embedding=embeddings)

def get_conversation_chain(vectorstore):
    """Set up the conversational AI chain with memory."""
    llm = ChatGoogleGenerativeAI(
        model='gemini-1.5-pro-latest',
        api_key=GOOGLE_API_KEY  # Use API key from environment variable
    )
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory
    )

def handle_userinput(question):
    """Process user queries and generate responses."""
    response = st.session_state.conversation({"question": question})
    st.session_state.chat_history = response['chat_history']

    st.write(f"**Question:** {question}")
    st.write(f"**Answer:** {response['answer']}")

def main():
    st.set_page_config(page_title="Lecture Chatbot", page_icon=":books:")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    st.header("Lecture Chatbot :books:")

    # Load transcript and process (only once)
    if st.session_state.conversation is None:
        with st.spinner("Loading transcript..."):
            raw_text = load_transcript()
            if raw_text:
                text_chunks = get_text_chunks(raw_text)
                vectorstore = get_vectorstore(text_chunks)
                st.session_state.conversation = get_conversation_chain(vectorstore)
                st.success("Transcript loaded successfully!")

    # User input for questions
    user_question = st.text_input("Ask a question about the lecture:")
    if user_question and st.session_state.conversation:
        handle_userinput(user_question)

if __name__ == '__main__':
    main()